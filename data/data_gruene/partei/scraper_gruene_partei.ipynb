{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.12.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (5.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.6)\n"
     ]
    }
   ],
   "source": [
    "#if you still need to install beautifulsoup4\n",
    "\n",
    "! pip install requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Seite 1...\n",
      "Scraping Seite 2...\n",
      "Scraping Seite 3...\n",
      "Scraping Seite 4...\n",
      "Scraping Seite 5...\n",
      "Scraping Seite 6...\n",
      "Scraping Seite 7...\n",
      "Scraping Seite 8...\n",
      "Scraping Seite 9...\n",
      "Scraping Seite 10...\n",
      "Scraping Seite 11...\n",
      "Scraping Seite 12...\n",
      "Scraping Seite 13...\n",
      "Scraping Seite 14...\n",
      "Scraping Seite 15...\n",
      "Scraping Seite 16...\n",
      "Scraping Seite 17...\n",
      "Scraping Seite 18...\n",
      "Scraping Seite 19...\n",
      "Scraping Seite 20...\n",
      "Scraping Seite 21...\n",
      "Scraping Seite 22...\n",
      "Scraping Seite 23...\n",
      "Scraping Seite 24...\n",
      "Scraping Seite 25...\n",
      "Scraping Seite 26...\n",
      "Scraping Seite 27...\n",
      "Scraping Seite 28...\n",
      "Scraping Seite 29...\n",
      "Scraping Seite 30...\n",
      "Scraping Seite 31...\n",
      "Scraping Seite 32...\n",
      "Scraping Seite 33...\n",
      "Scraping Seite 34...\n",
      "Scraping Seite 35...\n",
      "Scraping Seite 36...\n",
      "Scraping Seite 37...\n",
      "Scraping Seite 38...\n",
      "Scraping Seite 39...\n",
      "Scraping Seite 40...\n",
      "Scraping Seite 41...\n",
      "Scraping Seite 42...\n",
      "Scraping Seite 43...\n",
      "Scraping Seite 44...\n",
      "Scraping Seite 45...\n",
      "Scraping Seite 46...\n",
      "Scraping Seite 47...\n",
      "Scraping Seite 48...\n",
      "Scraping Seite 49...\n",
      "Scraping Seite 50...\n",
      "Scraping Seite 51...\n",
      "Scraping Seite 52...\n",
      "Scraping Seite 53...\n",
      "Scraping Seite 54...\n",
      "Scraping Seite 55...\n",
      "Scraping Seite 56...\n",
      "Scraping Seite 57...\n",
      "Scraping Seite 58...\n",
      "Scraping Seite 59...\n",
      "Scraping Seite 60...\n",
      "Scraping Seite 61...\n",
      "Scraping Seite 62...\n",
      "Scraping Seite 63...\n",
      "Scraping Seite 64...\n",
      "Scraping Seite 65...\n",
      "Scraping Seite 66...\n",
      "Scraping Seite 67...\n",
      "Scraping Seite 68...\n",
      "Scraping Seite 69...\n",
      "Scraping Seite 70...\n",
      "Scraping Seite 71...\n",
      "Scraping Seite 72...\n",
      "Scraping Seite 73...\n",
      "Scraping Seite 74...\n",
      "Scraping Seite 75...\n",
      "Scraping Seite 76...\n",
      "Scraping Seite 77...\n",
      "Scraping Seite 78...\n",
      "Scraping Seite 79...\n",
      "Scraping Seite 80...\n",
      "Scraping Seite 81...\n",
      "Scraping Seite 82...\n",
      "Scraping Seite 83...\n",
      "Scraping Seite 84...\n",
      "Scraping Seite 85...\n",
      "Scraping Seite 86...\n",
      "Scraping Seite 87...\n",
      "Scraping Seite 88...\n",
      "Scraping Seite 89...\n",
      "Scraping Seite 90...\n",
      "Scraping Seite 91...\n",
      "Scraping Seite 92...\n",
      "Scraping Seite 93...\n",
      "Scraping Seite 94...\n",
      "Scraping Seite 95...\n",
      "Scraping Seite 96...\n",
      "Scraping Seite 97...\n",
      "Scraping Seite 98...\n",
      "Scraping Seite 99...\n",
      "Scraping Seite 100...\n",
      "Daten erfolgreich gespeichert in ots_scraper_data_gruene.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Basis-URL\n",
    "base_url = \"https://www.ots.at/suche\"\n",
    "\n",
    "# Angepasste Anfrage-Parameter\n",
    "params = {\n",
    "    \"query\": \"\",\n",
    "    \"seite\": 1,\n",
    "    \"emittentId\": 14446,  # Angepasste Emittent-ID\n",
    "    \"startDate\": 1546815601,  # Angepasster Startzeitpunkt (Unix-Zeitstempel)\n",
    "    \"endDate\": 1610060399,    # Angepasster Endzeitpunkt (Unix-Zeitstempel)\n",
    "    \"channel\": \"index\",\n",
    "    \"attachment\": \"\"\n",
    "}\n",
    "\n",
    "# Funktion zum Abrufen von Artikeldetails\n",
    "def scrape_article_content(article_url):\n",
    "    try:\n",
    "        response = requests.get(article_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Fehler beim Abrufen der Artikel-URL: {article_url}\")\n",
    "            return None, None, None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Titel\n",
    "        title = soup.find(\"h1\").get_text(strip=True)\n",
    "        # Inhalt\n",
    "        content_div = soup.find(\"div\", {\"itemprop\": \"articleBody\"})\n",
    "        content = content_div.get_text(strip=True) if content_div else \"Kein Inhalt verfügbar\"\n",
    "        # Veröffentlichungsdatum\n",
    "        date_meta = soup.find(\"meta\", itemprop=\"datePublished\")\n",
    "        date_time = soup.find(\"time\", {\"itemprop\": \"datePublished\"})\n",
    "        date = (\n",
    "            date_meta[\"content\"] if date_meta and date_meta.has_attr(\"content\") \n",
    "            else date_time[\"datetime\"] if date_time and date_time.has_attr(\"datetime\") \n",
    "            else date_time.get_text(strip=True) if date_time \n",
    "            else \"Kein Datum verfügbar\"\n",
    "        )\n",
    "        return title, content, date\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Abrufen der Details: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Funktion zum Scrapen einer Seite\n",
    "def scrape_page(page_number):\n",
    "    params[\"seite\"] = page_number\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Fehler beim Abrufen der Seite {page_number}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"mat-card\")\n",
    "    data = []\n",
    "\n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Titel\n",
    "            title_tag = article.find(\"h1\", class_=\"display-3\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"Kein Titel\"\n",
    "\n",
    "            # Teaser\n",
    "            teaser_tag = article.find(\"p\", class_=\"lead\")\n",
    "            teaser = teaser_tag.get_text(strip=True) if teaser_tag else \"Kein Teaser\"\n",
    "\n",
    "            # Link\n",
    "            link_tag = article.find(\"a\", class_=\"link-detailed-view\")\n",
    "            link = f\"https://www.ots.at{link_tag['href']}\" if link_tag else \"Kein Link\"\n",
    "\n",
    "            # Details scrapen\n",
    "            article_title, article_content, article_date = scrape_article_content(link)\n",
    "\n",
    "            # Daten hinzufügen\n",
    "            data.append({\n",
    "                \"Titel\": title,\n",
    "                \"Teaser\": teaser,\n",
    "                \"Datum\": article_date,\n",
    "                \"Link\": link,\n",
    "                \"Inhalt\": article_content\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten eines Artikels: {e}\")\n",
    "            continue\n",
    "\n",
    "    return data\n",
    "\n",
    "# Hauptfunktion für mehrere Seiten\n",
    "def scrape_all_pages(start_page=1, end_page=5):\n",
    "    all_data = []\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        print(f\"Scraping Seite {page}...\")\n",
    "        page_data = scrape_page(page)\n",
    "        all_data.extend(page_data)\n",
    "        time.sleep(2)  # Wartezeit zwischen Anfragen\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Ergebnisse speichern\n",
    "def save_to_csv(data, filename=\"ots_scraper_data_gruene_partei.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Daten erfolgreich gespeichert in {filename}\")\n",
    "\n",
    "# Ausführung\n",
    "if __name__ == \"__main__\":\n",
    "    scraped_data = scrape_all_pages(start_page=1, end_page=100)  \n",
    "    if scraped_data:\n",
    "        save_to_csv(scraped_data)\n",
    "    else:\n",
    "        print(\"Keine Daten gescrapt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titel</th>\n",
       "      <th>Teaser</th>\n",
       "      <th>Datum</th>\n",
       "      <th>Link</th>\n",
       "      <th>Inhalt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ernst-Dziedzic zu Washington-Krawallen: Freie ...</td>\n",
       "      <td>Grüne verurteilen gewaltsame Stürmung des Kapi...</td>\n",
       "      <td>2021-01-07T10:35:35+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_202101...</td>\n",
       "      <td>Grüne verurteilen gewaltsame Stürmung des Kapi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grüne/Weratschnig: Lehren aus der Covid-Krise ...</td>\n",
       "      <td>2020 wurden 1,8 Milliarden Liter weniger Treib...</td>\n",
       "      <td>2021-01-05T13:19:08+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_202101...</td>\n",
       "      <td>2020 wurden 1,8 Milliarden Liter weniger Treib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grüne begrüßen britische Ablehnung des US-Ausl...</td>\n",
       "      <td>Ewa Ernst-Dziedzic: Weitergehende Maßnahmen zu...</td>\n",
       "      <td>2021-01-04T15:56:57+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_202101...</td>\n",
       "      <td>Ewa Ernst-Dziedzic: Weitergehende Maßnahmen zu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kogler und Stoytchev danken Birgit Hebein</td>\n",
       "      <td>„Hebein hat Klimaschutz- &amp; Sozialpolitik in be...</td>\n",
       "      <td>2020-12-30T13:00:09+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_202012...</td>\n",
       "      <td>„Hebein hat Klimaschutz- &amp; Sozialpolitik in be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grüne/Litschauer: AKW Krško ist weiterhin eine...</td>\n",
       "      <td>Das Atomkraftwerk muss nach Erdbeben endgültig...</td>\n",
       "      <td>2020-12-30T11:25:19+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_202012...</td>\n",
       "      <td>Das Atomkraftwerk muss nach Erdbeben endgültig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>Anschober/Zadic: Wichtiger Schritt für rasche ...</td>\n",
       "      <td>Parlamentsbeschluss für Abschiebestopp während...</td>\n",
       "      <td>2019-11-05T16:22:28+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_201911...</td>\n",
       "      <td>Parlamentsbeschluss für Abschiebestopp während...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>Kogler/Gewessler: Jede künftige Regierung wird...</td>\n",
       "      <td>Grüne begrüßen Volksbegehrens-Initiative, die ...</td>\n",
       "      <td>2019-11-05T13:58:31+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_201911...</td>\n",
       "      <td>Grüne begrüßen Volksbegehrens-Initiative, die ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>Gewessler/Hammer: Nationaler Klima- und Energi...</td>\n",
       "      <td>Nur minimale Verbesserungen im Vergleich zu er...</td>\n",
       "      <td>2019-11-04T12:34:00+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_201911...</td>\n",
       "      <td>Nur minimale Verbesserungen im Vergleich zu er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>Ernst-Dziedzic zu Liederbuch: Rücktritt von Za...</td>\n",
       "      <td>Grüne: Liederbuch-Affäre Teil 3 zeigt, Distanz...</td>\n",
       "      <td>2019-10-31T10:44:22+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_201910...</td>\n",
       "      <td>Grüne: Liederbuch-Affäre Teil 3 zeigt, Distanz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>Ernst-Dziedzic zu \"Original Play\": Rasche Aufk...</td>\n",
       "      <td>Grüne: Vereine in Kindergärten und Schulen müs...</td>\n",
       "      <td>2019-10-25T14:20:56+02:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_201910...</td>\n",
       "      <td>Grüne: Vereine in Kindergärten und Schulen müs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>923 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Titel  \\\n",
       "0    Ernst-Dziedzic zu Washington-Krawallen: Freie ...   \n",
       "1    Grüne/Weratschnig: Lehren aus der Covid-Krise ...   \n",
       "2    Grüne begrüßen britische Ablehnung des US-Ausl...   \n",
       "3            Kogler und Stoytchev danken Birgit Hebein   \n",
       "4    Grüne/Litschauer: AKW Krško ist weiterhin eine...   \n",
       "..                                                 ...   \n",
       "918  Anschober/Zadic: Wichtiger Schritt für rasche ...   \n",
       "919  Kogler/Gewessler: Jede künftige Regierung wird...   \n",
       "920  Gewessler/Hammer: Nationaler Klima- und Energi...   \n",
       "921  Ernst-Dziedzic zu Liederbuch: Rücktritt von Za...   \n",
       "922  Ernst-Dziedzic zu \"Original Play\": Rasche Aufk...   \n",
       "\n",
       "                                                Teaser  \\\n",
       "0    Grüne verurteilen gewaltsame Stürmung des Kapi...   \n",
       "1    2020 wurden 1,8 Milliarden Liter weniger Treib...   \n",
       "2    Ewa Ernst-Dziedzic: Weitergehende Maßnahmen zu...   \n",
       "3    „Hebein hat Klimaschutz- & Sozialpolitik in be...   \n",
       "4    Das Atomkraftwerk muss nach Erdbeben endgültig...   \n",
       "..                                                 ...   \n",
       "918  Parlamentsbeschluss für Abschiebestopp während...   \n",
       "919  Grüne begrüßen Volksbegehrens-Initiative, die ...   \n",
       "920  Nur minimale Verbesserungen im Vergleich zu er...   \n",
       "921  Grüne: Liederbuch-Affäre Teil 3 zeigt, Distanz...   \n",
       "922  Grüne: Vereine in Kindergärten und Schulen müs...   \n",
       "\n",
       "                         Datum  \\\n",
       "0    2021-01-07T10:35:35+01:00   \n",
       "1    2021-01-05T13:19:08+01:00   \n",
       "2    2021-01-04T15:56:57+01:00   \n",
       "3    2020-12-30T13:00:09+01:00   \n",
       "4    2020-12-30T11:25:19+01:00   \n",
       "..                         ...   \n",
       "918  2019-11-05T16:22:28+01:00   \n",
       "919  2019-11-05T13:58:31+01:00   \n",
       "920  2019-11-04T12:34:00+01:00   \n",
       "921  2019-10-31T10:44:22+01:00   \n",
       "922  2019-10-25T14:20:56+02:00   \n",
       "\n",
       "                                                  Link  \\\n",
       "0    https://www.ots.at/presseaussendung/OTS_202101...   \n",
       "1    https://www.ots.at/presseaussendung/OTS_202101...   \n",
       "2    https://www.ots.at/presseaussendung/OTS_202101...   \n",
       "3    https://www.ots.at/presseaussendung/OTS_202012...   \n",
       "4    https://www.ots.at/presseaussendung/OTS_202012...   \n",
       "..                                                 ...   \n",
       "918  https://www.ots.at/presseaussendung/OTS_201911...   \n",
       "919  https://www.ots.at/presseaussendung/OTS_201911...   \n",
       "920  https://www.ots.at/presseaussendung/OTS_201911...   \n",
       "921  https://www.ots.at/presseaussendung/OTS_201910...   \n",
       "922  https://www.ots.at/presseaussendung/OTS_201910...   \n",
       "\n",
       "                                                Inhalt  \n",
       "0    Grüne verurteilen gewaltsame Stürmung des Kapi...  \n",
       "1    2020 wurden 1,8 Milliarden Liter weniger Treib...  \n",
       "2    Ewa Ernst-Dziedzic: Weitergehende Maßnahmen zu...  \n",
       "3    „Hebein hat Klimaschutz- & Sozialpolitik in be...  \n",
       "4    Das Atomkraftwerk muss nach Erdbeben endgültig...  \n",
       "..                                                 ...  \n",
       "918  Parlamentsbeschluss für Abschiebestopp während...  \n",
       "919  Grüne begrüßen Volksbegehrens-Initiative, die ...  \n",
       "920  Nur minimale Verbesserungen im Vergleich zu er...  \n",
       "921  Grüne: Liederbuch-Affäre Teil 3 zeigt, Distanz...  \n",
       "922  Grüne: Vereine in Kindergärten und Schulen müs...  \n",
       "\n",
       "[923 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_gruene = pd.read_csv(\"ots_scraper_data_gruene_partei.csv\")\n",
    "\n",
    "display(df_gruene)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
