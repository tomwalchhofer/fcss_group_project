{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.12.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (5.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\walchhth\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.6)\n"
     ]
    }
   ],
   "source": [
    "#if you still need to install beautifulsoup4\n",
    "\n",
    "! pip install requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Basis-URL\n",
    "base_url = \"https://www.ots.at/suche\"\n",
    "\n",
    "# Anfrage-Parameter\n",
    "params = {\n",
    "    \"query\": \"\",\n",
    "    \"seite\": 1,\n",
    "    \"emittentId\": 195,  # Emittent-ID\n",
    "    \"startDate\": 1136934001,  # Startdatum\n",
    "    \"endDate\": 1200092399,    # Enddatum\n",
    "    \"channel\": \"index\",\n",
    "    \"attachment\": \"\"\n",
    "}\n",
    "\n",
    "# Funktion zum Abrufen von Artikeldetails\n",
    "def scrape_article_content(article_url):\n",
    "    try:\n",
    "        response = requests.get(article_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Fehler beim Abrufen der Artikel-URL: {article_url}\")\n",
    "            return None, None, None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Titel\n",
    "        title = soup.find(\"h1\").get_text(strip=True)\n",
    "        # Inhalt\n",
    "        content_div = soup.find(\"div\", {\"itemprop\": \"articleBody\"})\n",
    "        content = content_div.get_text(strip=True) if content_div else \"Kein Inhalt verfügbar\"\n",
    "        # Veröffentlichungsdatum\n",
    "        date_meta = soup.find(\"meta\", itemprop=\"datePublished\")\n",
    "        date_time = soup.find(\"time\", {\"itemprop\": \"datePublished\"})\n",
    "        date = (\n",
    "            date_meta[\"content\"] if date_meta and date_meta.has_attr(\"content\") \n",
    "            else date_time[\"datetime\"] if date_time and date_time.has_attr(\"datetime\") \n",
    "            else date_time.get_text(strip=True) if date_time \n",
    "            else \"Kein Datum verfügbar\"\n",
    "        )\n",
    "        return title, content, date\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Abrufen der Details: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Funktion zum Scrapen einer Seite\n",
    "def scrape_page(page_number):\n",
    "    params[\"seite\"] = page_number\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Fehler beim Abrufen der Seite {page_number}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"mat-card\")\n",
    "    data = []\n",
    "\n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Titel\n",
    "            title_tag = article.find(\"h1\", class_=\"display-3\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"Kein Titel\"\n",
    "\n",
    "            # Teaser\n",
    "            teaser_tag = article.find(\"p\", class_=\"lead\")\n",
    "            teaser = teaser_tag.get_text(strip=True) if teaser_tag else \"Kein Teaser\"\n",
    "\n",
    "            # Link\n",
    "            link_tag = article.find(\"a\", class_=\"link-detailed-view\")\n",
    "            link = f\"https://www.ots.at{link_tag['href']}\" if link_tag else \"Kein Link\"\n",
    "\n",
    "            # Details scrapen\n",
    "            article_title, article_content, article_date = scrape_article_content(link)\n",
    "\n",
    "            # Daten hinzufügen\n",
    "            data.append({\n",
    "                \"Titel\": title,\n",
    "                \"Teaser\": teaser,\n",
    "                \"Datum\": article_date,\n",
    "                \"Link\": link,\n",
    "                \"Inhalt\": article_content\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten eines Artikels: {e}\")\n",
    "            continue\n",
    "\n",
    "    return data\n",
    "\n",
    "# Hauptfunktion für mehrere Seiten\n",
    "def scrape_all_pages(start_page=1, end_page=5):\n",
    "    all_data = []\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        print(f\"Scraping Seite {page}...\")\n",
    "        page_data = scrape_page(page)\n",
    "        all_data.extend(page_data)\n",
    "        time.sleep(2)  # Wartezeit zwischen Anfragen\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Ergebnisse speichern\n",
    "def save_to_csv(data, filename=\"ots_scraper_data_spoe_partei.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Daten erfolgreich gespeichert in {filename}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping pages 1 to 50...\n",
      "Scraping Seite 1...\n",
      "Scraping Seite 2...\n",
      "Scraping Seite 3...\n",
      "Scraping Seite 4...\n",
      "Scraping Seite 5...\n",
      "Scraping Seite 6...\n",
      "Scraping Seite 7...\n",
      "Scraping Seite 8...\n",
      "Scraping Seite 9...\n",
      "Scraping Seite 10...\n",
      "Scraping Seite 11...\n",
      "Scraping Seite 12...\n",
      "Scraping Seite 13...\n",
      "Scraping Seite 14...\n",
      "Scraping Seite 15...\n",
      "Scraping Seite 16...\n",
      "Scraping Seite 17...\n",
      "Scraping Seite 18...\n",
      "Scraping Seite 19...\n",
      "Scraping Seite 20...\n",
      "Scraping Seite 21...\n",
      "Scraping Seite 22...\n",
      "Scraping Seite 23...\n",
      "Scraping Seite 24...\n",
      "Scraping Seite 25...\n",
      "Scraping Seite 26...\n",
      "Scraping Seite 27...\n",
      "Scraping Seite 28...\n",
      "Scraping Seite 29...\n",
      "Scraping Seite 30...\n",
      "Scraping Seite 31...\n",
      "Scraping Seite 32...\n",
      "Scraping Seite 33...\n",
      "Scraping Seite 34...\n",
      "Scraping Seite 35...\n",
      "Scraping Seite 36...\n",
      "Scraping Seite 37...\n",
      "Scraping Seite 38...\n",
      "Scraping Seite 39...\n",
      "Scraping Seite 40...\n",
      "Scraping Seite 41...\n",
      "Scraping Seite 42...\n",
      "Scraping Seite 43...\n",
      "Scraping Seite 44...\n",
      "Scraping Seite 45...\n",
      "Scraping Seite 46...\n",
      "Scraping Seite 47...\n",
      "Scraping Seite 48...\n",
      "Scraping Seite 49...\n",
      "Scraping Seite 50...\n",
      "Daten erfolgreich gespeichert in ots_scraper_data_spoe_partei_part_1.csv\n",
      "Scraping pages 51 to 100...\n",
      "Scraping Seite 51...\n",
      "Scraping Seite 52...\n",
      "Scraping Seite 53...\n",
      "Scraping Seite 54...\n",
      "Scraping Seite 55...\n",
      "Scraping Seite 56...\n",
      "Scraping Seite 57...\n",
      "Scraping Seite 58...\n",
      "Scraping Seite 59...\n",
      "Scraping Seite 60...\n",
      "Scraping Seite 61...\n",
      "Scraping Seite 62...\n",
      "Scraping Seite 63...\n",
      "Scraping Seite 64...\n",
      "Scraping Seite 65...\n",
      "Scraping Seite 66...\n",
      "Scraping Seite 67...\n",
      "Scraping Seite 68...\n",
      "Scraping Seite 69...\n",
      "Scraping Seite 70...\n",
      "Scraping Seite 71...\n",
      "Scraping Seite 72...\n",
      "Scraping Seite 73...\n",
      "Scraping Seite 74...\n",
      "Scraping Seite 75...\n",
      "Scraping Seite 76...\n",
      "Scraping Seite 77...\n",
      "Scraping Seite 78...\n",
      "Scraping Seite 79...\n",
      "Scraping Seite 80...\n",
      "Scraping Seite 81...\n",
      "Scraping Seite 82...\n",
      "Scraping Seite 83...\n",
      "Scraping Seite 84...\n",
      "Scraping Seite 85...\n",
      "Scraping Seite 86...\n",
      "Scraping Seite 87...\n",
      "Scraping Seite 88...\n",
      "Scraping Seite 89...\n",
      "Scraping Seite 90...\n",
      "Scraping Seite 91...\n",
      "Scraping Seite 92...\n",
      "Scraping Seite 93...\n",
      "Scraping Seite 94...\n",
      "Scraping Seite 95...\n",
      "Scraping Seite 96...\n",
      "Scraping Seite 97...\n",
      "Scraping Seite 98...\n",
      "Scraping Seite 99...\n",
      "Scraping Seite 100...\n",
      "Daten erfolgreich gespeichert in ots_scraper_data_spoe_partei_part_2.csv\n",
      "Scraping pages 101 to 150...\n",
      "Scraping Seite 101...\n",
      "Scraping Seite 102...\n",
      "Scraping Seite 103...\n",
      "Scraping Seite 104...\n",
      "Scraping Seite 105...\n",
      "Scraping Seite 106...\n",
      "Scraping Seite 107...\n",
      "Scraping Seite 108...\n",
      "Scraping Seite 109...\n",
      "Scraping Seite 110...\n",
      "Scraping Seite 111...\n",
      "Scraping Seite 112...\n",
      "Scraping Seite 113...\n",
      "Scraping Seite 114...\n",
      "Scraping Seite 115...\n",
      "Scraping Seite 116...\n",
      "Scraping Seite 117...\n",
      "Scraping Seite 118...\n",
      "Scraping Seite 119...\n",
      "Scraping Seite 120...\n",
      "Scraping Seite 121...\n",
      "Scraping Seite 122...\n",
      "Scraping Seite 123...\n",
      "Scraping Seite 124...\n",
      "Scraping Seite 125...\n",
      "Scraping Seite 126...\n",
      "Scraping Seite 127...\n",
      "Scraping Seite 128...\n",
      "Scraping Seite 129...\n",
      "Scraping Seite 130...\n",
      "Scraping Seite 131...\n",
      "Scraping Seite 132...\n",
      "Scraping Seite 133...\n",
      "Scraping Seite 134...\n",
      "Scraping Seite 135...\n",
      "Scraping Seite 136...\n",
      "Scraping Seite 137...\n",
      "Scraping Seite 138...\n",
      "Scraping Seite 139...\n",
      "Scraping Seite 140...\n",
      "Scraping Seite 141...\n",
      "Scraping Seite 142...\n",
      "Scraping Seite 143...\n",
      "Scraping Seite 144...\n",
      "Scraping Seite 145...\n",
      "Scraping Seite 146...\n",
      "Scraping Seite 147...\n",
      "Scraping Seite 148...\n",
      "Scraping Seite 149...\n",
      "Scraping Seite 150...\n",
      "Daten erfolgreich gespeichert in ots_scraper_data_spoe_partei_part_3.csv\n",
      "Scraping pages 151 to 200...\n",
      "Scraping Seite 151...\n",
      "Scraping Seite 152...\n",
      "Scraping Seite 153...\n",
      "Scraping Seite 154...\n",
      "Scraping Seite 155...\n",
      "Scraping Seite 156...\n",
      "Scraping Seite 157...\n",
      "Scraping Seite 158...\n",
      "Scraping Seite 159...\n",
      "Scraping Seite 160...\n",
      "Scraping Seite 161...\n",
      "Scraping Seite 162...\n",
      "Scraping Seite 163...\n",
      "Scraping Seite 164...\n",
      "Scraping Seite 165...\n",
      "Scraping Seite 166...\n",
      "Scraping Seite 167...\n",
      "Scraping Seite 168...\n",
      "Scraping Seite 169...\n",
      "Scraping Seite 170...\n",
      "Scraping Seite 171...\n",
      "Scraping Seite 172...\n",
      "Scraping Seite 173...\n",
      "Scraping Seite 174...\n",
      "Scraping Seite 175...\n",
      "Scraping Seite 176...\n",
      "Scraping Seite 177...\n",
      "Scraping Seite 178...\n",
      "Scraping Seite 179...\n",
      "Scraping Seite 180...\n",
      "Scraping Seite 181...\n",
      "Scraping Seite 182...\n",
      "Scraping Seite 183...\n",
      "Scraping Seite 184...\n",
      "Scraping Seite 185...\n",
      "Scraping Seite 186...\n",
      "Scraping Seite 187...\n",
      "Scraping Seite 188...\n",
      "Scraping Seite 189...\n",
      "Scraping Seite 190...\n",
      "Scraping Seite 191...\n",
      "Scraping Seite 192...\n",
      "Scraping Seite 193...\n",
      "Scraping Seite 194...\n",
      "Scraping Seite 195...\n",
      "Scraping Seite 196...\n",
      "Scraping Seite 197...\n",
      "Scraping Seite 198...\n",
      "Scraping Seite 199...\n",
      "Scraping Seite 200...\n",
      "Daten erfolgreich gespeichert in ots_scraper_data_spoe_partei_part_4.csv\n",
      "Scraping pages 201 to 250...\n",
      "Scraping Seite 201...\n",
      "Scraping Seite 202...\n",
      "Scraping Seite 203...\n",
      "Scraping Seite 204...\n",
      "Scraping Seite 205...\n",
      "Scraping Seite 206...\n",
      "Scraping Seite 207...\n",
      "Scraping Seite 208...\n",
      "Scraping Seite 209...\n",
      "Scraping Seite 210...\n",
      "Scraping Seite 211...\n",
      "Scraping Seite 212...\n",
      "Scraping Seite 213...\n",
      "Scraping Seite 214...\n",
      "Scraping Seite 215...\n",
      "Scraping Seite 216...\n",
      "Scraping Seite 217...\n",
      "Scraping Seite 218...\n",
      "Scraping Seite 219...\n",
      "Scraping Seite 220...\n",
      "Scraping Seite 221...\n",
      "Scraping Seite 222...\n",
      "Scraping Seite 223...\n",
      "Scraping Seite 224...\n",
      "Scraping Seite 225...\n",
      "Scraping Seite 226...\n",
      "Scraping Seite 227...\n",
      "Scraping Seite 228...\n",
      "Scraping Seite 229...\n",
      "Scraping Seite 230...\n",
      "Scraping Seite 231...\n",
      "Scraping Seite 232...\n",
      "Scraping Seite 233...\n",
      "Scraping Seite 234...\n",
      "Scraping Seite 235...\n",
      "Scraping Seite 236...\n",
      "Scraping Seite 237...\n",
      "Scraping Seite 238...\n",
      "Scraping Seite 239...\n",
      "Scraping Seite 240...\n",
      "Scraping Seite 241...\n",
      "Scraping Seite 242...\n",
      "Scraping Seite 243...\n",
      "Scraping Seite 244...\n",
      "Scraping Seite 245...\n",
      "Scraping Seite 246...\n",
      "Scraping Seite 247...\n",
      "Scraping Seite 248...\n",
      "Scraping Seite 249...\n",
      "Scraping Seite 250...\n",
      "Daten erfolgreich gespeichert in ots_scraper_data_spoe_partei_part_5.csv\n"
     ]
    }
   ],
   "source": [
    "# Ausführung: Scrape data in steps\n",
    "for i in range(5):  # Adjust the range as needed to cover all steps\n",
    "    x = i * 50  # Calculate the starting page for this iteration\n",
    "    if __name__ == \"__main__\":\n",
    "        print(f\"Scraping pages {x + 1} to {x + 50}...\")\n",
    "        scraped_data = scrape_all_pages(start_page=x + 1, end_page=x + 50)  \n",
    "        if scraped_data:\n",
    "            # Save to a unique file for each iteration\n",
    "            filename = f\"ots_scraper_data_spoe_partei_part_{i + 1}.csv\"\n",
    "            save_to_csv(scraped_data, filename)\n",
    "        else:\n",
    "            print(f\"Keine Daten gescrapt für Seiten {x + 1} bis {x + 50}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titel</th>\n",
       "      <th>Teaser</th>\n",
       "      <th>Datum</th>\n",
       "      <th>Link</th>\n",
       "      <th>Inhalt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Matznetter besorgt: US-Hypothekenkrise ist noc...</td>\n",
       "      <td>Veranstaltung des Renner-Instituts zum Einflus...</td>\n",
       "      <td>2008-01-11T16:09:52+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_200801...</td>\n",
       "      <td>Veranstaltung des Renner-Instituts zum Einflus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cap: SPÖ-Forderung nach Rückforderungsverzicht...</td>\n",
       "      <td>Wien (SK) - Er begrüße, dass die Forderung der...</td>\n",
       "      <td>2008-01-11T15:31:39+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_200801...</td>\n",
       "      <td>Wien (SK) - Er begrüße, dass die Forderung der...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gusenbauer: Zur Pflegelösung kommt \"Aktion Sch...</td>\n",
       "      <td>Hacklerregelung bis 2013 verlängert - Pensions...</td>\n",
       "      <td>2008-01-11T15:21:04+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_200801...</td>\n",
       "      <td>Hacklerregelung bis 2013 verlängert - Pensions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kalina: Kanzler Gusenbauer zeigt Führungs- und...</td>\n",
       "      <td>Hacklerregelung \"großer Erfolg für tausende ha...</td>\n",
       "      <td>2008-01-11T15:08:17+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_200801...</td>\n",
       "      <td>Hacklerregelung \"großer Erfolg für tausende ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio-OTS von Bundeskanzler Gusenbauer auf www...</td>\n",
       "      <td>Wien (SK) - Der Pressedienst der SPÖ stellt in...</td>\n",
       "      <td>2008-01-11T14:33:54+01:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_200801...</td>\n",
       "      <td>Wien (SK) - Der Pressedienst der SPÖ stellt in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>SPÖ-Termine vom 1. Oktober bis 7. Oktober 2007</td>\n",
       "      <td>Wien (SK) -</td>\n",
       "      <td>2007-09-28T11:32:13+02:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_200709...</td>\n",
       "      <td>Wien (SK) -MONTAG, 1. Oktober 2007:Finanzstaat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Heute Freitag: SPÖ-Bundesgeschäftsführer Kalin...</td>\n",
       "      <td>Wien (SK) - Wir erlauben uns, die VertreterInn...</td>\n",
       "      <td>2007-09-28T09:10:40+02:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_200709...</td>\n",
       "      <td>Wien (SK) - Wir erlauben uns, die VertreterInn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Gusenbauer: Gemeinsames Vorangehen von Europa ...</td>\n",
       "      <td>Transatlantische Beziehungen neu definieren</td>\n",
       "      <td>2007-09-27T16:01:10+02:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_200709...</td>\n",
       "      <td>Transatlantische Beziehungen neu definierenWie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Kalina: Altes Kabinett Schüssel Kabarettnummer</td>\n",
       "      <td>Reaktion Molterers auf Gorbachs Briefkopf-Affä...</td>\n",
       "      <td>2007-09-27T15:10:06+02:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_200709...</td>\n",
       "      <td>Reaktion Molterers auf Gorbachs Briefkopf-Affä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Berger präsentiert neue Leiterin der Justizans...</td>\n",
       "      <td>Justizministerin kündigt neue Pilotprojekte mi...</td>\n",
       "      <td>2007-09-27T12:36:39+02:00</td>\n",
       "      <td>https://www.ots.at/presseaussendung/OTS_200709...</td>\n",
       "      <td>Justizministerin kündigt neue Pilotprojekte mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Titel  \\\n",
       "0    Matznetter besorgt: US-Hypothekenkrise ist noc...   \n",
       "1    Cap: SPÖ-Forderung nach Rückforderungsverzicht...   \n",
       "2    Gusenbauer: Zur Pflegelösung kommt \"Aktion Sch...   \n",
       "3    Kalina: Kanzler Gusenbauer zeigt Führungs- und...   \n",
       "4    Audio-OTS von Bundeskanzler Gusenbauer auf www...   \n",
       "..                                                 ...   \n",
       "495     SPÖ-Termine vom 1. Oktober bis 7. Oktober 2007   \n",
       "496  Heute Freitag: SPÖ-Bundesgeschäftsführer Kalin...   \n",
       "497  Gusenbauer: Gemeinsames Vorangehen von Europa ...   \n",
       "498     Kalina: Altes Kabinett Schüssel Kabarettnummer   \n",
       "499  Berger präsentiert neue Leiterin der Justizans...   \n",
       "\n",
       "                                                Teaser  \\\n",
       "0    Veranstaltung des Renner-Instituts zum Einflus...   \n",
       "1    Wien (SK) - Er begrüße, dass die Forderung der...   \n",
       "2    Hacklerregelung bis 2013 verlängert - Pensions...   \n",
       "3    Hacklerregelung \"großer Erfolg für tausende ha...   \n",
       "4    Wien (SK) - Der Pressedienst der SPÖ stellt in...   \n",
       "..                                                 ...   \n",
       "495                                        Wien (SK) -   \n",
       "496  Wien (SK) - Wir erlauben uns, die VertreterInn...   \n",
       "497        Transatlantische Beziehungen neu definieren   \n",
       "498  Reaktion Molterers auf Gorbachs Briefkopf-Affä...   \n",
       "499  Justizministerin kündigt neue Pilotprojekte mi...   \n",
       "\n",
       "                         Datum  \\\n",
       "0    2008-01-11T16:09:52+01:00   \n",
       "1    2008-01-11T15:31:39+01:00   \n",
       "2    2008-01-11T15:21:04+01:00   \n",
       "3    2008-01-11T15:08:17+01:00   \n",
       "4    2008-01-11T14:33:54+01:00   \n",
       "..                         ...   \n",
       "495  2007-09-28T11:32:13+02:00   \n",
       "496  2007-09-28T09:10:40+02:00   \n",
       "497  2007-09-27T16:01:10+02:00   \n",
       "498  2007-09-27T15:10:06+02:00   \n",
       "499  2007-09-27T12:36:39+02:00   \n",
       "\n",
       "                                                  Link  \\\n",
       "0    https://www.ots.at/presseaussendung/OTS_200801...   \n",
       "1    https://www.ots.at/presseaussendung/OTS_200801...   \n",
       "2    https://www.ots.at/presseaussendung/OTS_200801...   \n",
       "3    https://www.ots.at/presseaussendung/OTS_200801...   \n",
       "4    https://www.ots.at/presseaussendung/OTS_200801...   \n",
       "..                                                 ...   \n",
       "495  https://www.ots.at/presseaussendung/OTS_200709...   \n",
       "496  https://www.ots.at/presseaussendung/OTS_200709...   \n",
       "497  https://www.ots.at/presseaussendung/OTS_200709...   \n",
       "498  https://www.ots.at/presseaussendung/OTS_200709...   \n",
       "499  https://www.ots.at/presseaussendung/OTS_200709...   \n",
       "\n",
       "                                                Inhalt  \n",
       "0    Veranstaltung des Renner-Instituts zum Einflus...  \n",
       "1    Wien (SK) - Er begrüße, dass die Forderung der...  \n",
       "2    Hacklerregelung bis 2013 verlängert - Pensions...  \n",
       "3    Hacklerregelung \"großer Erfolg für tausende ha...  \n",
       "4    Wien (SK) - Der Pressedienst der SPÖ stellt in...  \n",
       "..                                                 ...  \n",
       "495  Wien (SK) -MONTAG, 1. Oktober 2007:Finanzstaat...  \n",
       "496  Wien (SK) - Wir erlauben uns, die VertreterInn...  \n",
       "497  Transatlantische Beziehungen neu definierenWie...  \n",
       "498  Reaktion Molterers auf Gorbachs Briefkopf-Affä...  \n",
       "499  Justizministerin kündigt neue Pilotprojekte mi...  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spoe = pd.read_csv(\"ots_scraper_data_spoe_partei_part_1.csv\")\n",
    "\n",
    "display(df_spoe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeite Datei: c:\\Users\\walchhth\\OneDrive - Styria-IT Solutions GmbH & Co KG\\Dokumente\\KLZ - TOM\\CSS - Master\\1. Semester\\Foundations of CSS\\Group_Project\\data\\data_spoe\\partei\\ots_scraper_data_spoe_partei_part_1.csv\n",
      "Verarbeite Datei: c:\\Users\\walchhth\\OneDrive - Styria-IT Solutions GmbH & Co KG\\Dokumente\\KLZ - TOM\\CSS - Master\\1. Semester\\Foundations of CSS\\Group_Project\\data\\data_spoe\\partei\\ots_scraper_data_spoe_partei_part_2.csv\n",
      "Verarbeite Datei: c:\\Users\\walchhth\\OneDrive - Styria-IT Solutions GmbH & Co KG\\Dokumente\\KLZ - TOM\\CSS - Master\\1. Semester\\Foundations of CSS\\Group_Project\\data\\data_spoe\\partei\\ots_scraper_data_spoe_partei_part_3.csv\n",
      "Verarbeite Datei: c:\\Users\\walchhth\\OneDrive - Styria-IT Solutions GmbH & Co KG\\Dokumente\\KLZ - TOM\\CSS - Master\\1. Semester\\Foundations of CSS\\Group_Project\\data\\data_spoe\\partei\\ots_scraper_data_spoe_partei_part_4.csv\n",
      "Verarbeite Datei: c:\\Users\\walchhth\\OneDrive - Styria-IT Solutions GmbH & Co KG\\Dokumente\\KLZ - TOM\\CSS - Master\\1. Semester\\Foundations of CSS\\Group_Project\\data\\data_spoe\\partei\\ots_scraper_data_spoe_partei_part_5.csv\n",
      "Alle Dateien erfolgreich kombiniert in: ots_scraper_data_spoe_partei.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Name der Ausgabe-Datei\n",
    "output_filename = \"ots_scraper_data_spoe_partei.csv\"\n",
    "\n",
    "def combine_csv_files(output_filename):\n",
    "    # Aktuelles Verzeichnis des Notebooks\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # Liste, um alle DataFrames zu speichern\n",
    "    all_data = []\n",
    "\n",
    "    # Iteriere durch alle Dateien im aktuellen Verzeichnis\n",
    "    for filename in os.listdir(current_directory):\n",
    "        if filename.startswith(\"ots_scraper_data_spoe_partei_part_\") and filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(current_directory, filename)\n",
    "            print(f\"Verarbeite Datei: {file_path}\")\n",
    "            # Lade die CSV-Datei und füge sie zur Liste hinzu\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_data.append(df)\n",
    "\n",
    "    # Kombiniere alle DataFrames in ein einziges DataFrame\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        # Speichere das kombinierte DataFrame in einer neuen CSV-Datei\n",
    "        combined_df.to_csv(output_filename, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Alle Dateien erfolgreich kombiniert in: {output_filename}\")\n",
    "    else:\n",
    "        print(\"Keine Dateien gefunden, die kombiniert werden können.\")\n",
    "\n",
    "# Funktion ausführen\n",
    "combine_csv_files(output_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
